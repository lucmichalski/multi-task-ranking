{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import BinaryType, StringType, ArrayType\n",
    "from pyspark.sql.functions import udf, row_number, monotonically_increasing_id, col, collect_list, concat_ws, explode\n",
    "from pyspark.sql import SparkSession, Window\n",
    "\n",
    "from protocol_buffers import document_pb2\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_drive_gbs = 3\n",
    "spark_executor_gbs = 50\n",
    "cores = 14\n",
    "\n",
    "para_path = '/nfs/trec_car/data/test_entity/full_data_v3_with_datasets_contents_v4/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n//////// RUNNING WITH CORES {} //////////'.format(cores))\n",
    "spark = SparkSession.\\\n",
    "    builder\\\n",
    "    .appName('test')\\\n",
    "    .master('local[{}]'.format(cores)) \\\n",
    "    .config(\"spark.driver.memory\", '{}g'.format(spark_drive_gbs)) \\\n",
    "    .config(\"spark.executor.memory\", '{}g'.format(spark_executor_gbs)) \\\n",
    "    .config(\"spark.driver.maxResultSize\", '{}g'.format(spark_drive_gbs)) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_metadata = {\n",
    "    'entity_train':\n",
    "        (\n",
    "        '/nfs/trec_car/data/entity_ranking/multi_task_data/entity_train_all_queries_BM25_1000.run',\n",
    "        '/nfs/trec_car/data/entity_ranking/benchmarkY1_hierarchical_entity_train_data/benchmarkY1_train_entity.qrels'),\n",
    "\n",
    "    'entity_dev':\n",
    "        ('/nfs/trec_car/data/entity_ranking/multi_task_data/entity_dev_all_queries_BM25_1000.run',\n",
    "         '/nfs/trec_car/data/entity_ranking/benchmarkY1_hierarchical_entity_dev_data/benchmarkY1_dev_entity.qrels'),\n",
    "\n",
    "    'entity_test':\n",
    "        ('/nfs/trec_car/data/entity_ranking/multi_task_data/entity_test_all_queries_BM25_1000.run',\n",
    "         '/nfs/trec_car/data/entity_ranking/testY1_hierarchical_entity_data/testY1_hierarchical_entity.qrels'),\n",
    "\n",
    "    'passage_train':\n",
    "        (\n",
    "        '/nfs/trec_car/data/entity_ranking/benchmarkY1_hierarchical_passage_train_data/benchmarkY1_train_passage_1000.run',\n",
    "        '/nfs/trec_car/data/entity_ranking/benchmarkY1_hierarchical_passage_train_data/benchmarkY1_train_passage.qrels'),\n",
    "\n",
    "    'passage_dev':\n",
    "        ('/nfs/trec_car/data/entity_ranking/benchmarkY1_hierarchical_passage_dev_data/benchmarkY1_dev_passage_1000.run',\n",
    "         '/nfs/trec_car/data/entity_ranking/benchmarkY1_hierarchical_passage_dev_data/benchmarkY1_dev_passage.qrels'),\n",
    "\n",
    "    'passage_test':\n",
    "        ('/nfs/trec_car/data/entity_ranking/testY1_hierarchical_passage_data/testY1_hierarchical_passage_1000.run',\n",
    "         '/nfs/trec_car/data/entity_ranking/testY1_hierarchical_passage_data/testY1_hierarchical_passage.qrels')\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(para_path)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=ArrayType(StringType()))\n",
    "def get_ents(content_bytearray):\n",
    "    synthetic_entity_links = document_pb2.DocumentContent().FromString(pickle.loads(content_bytearray)).synthetic_entity_links\n",
    "    entity_links = []\n",
    "    for synthetic_entity_link in synthetic_entity_links:\n",
    "        entity_links.append(str(synthetic_entity_link.entity_id))\n",
    "    return entity_links\n",
    "\n",
    "df_entity = df.withColumn(\"entities\", get_ents(\"content_bytearray\"))\n",
    "df_entity.printSchema()\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/nfs/trec_car/data/entity_ranking/multi_task_data_by_query/'\n",
    "max_rank = 100\n",
    "\n",
    "for dataset in ['dev', 'test', 'train']:\n",
    "    dateset_dir = base_dir + '{}_data/'.format(dataset)\n",
    "    passage_name = 'passage' + '_{}'.format(dataset)\n",
    "    passage_path = dataset_metadata[passage_name][0]\n",
    "    \n",
    "    print('Building passage->entity mappings for {}: {}'.format(dataset, passage_path))\n",
    "    run_dict = {}\n",
    "    doc_ids_list = []\n",
    "    with open(passage_path, 'r') as f:\n",
    "        for line in f:\n",
    "                \n",
    "            query = line.split()[0]\n",
    "            doc_id = line.split()[2]\n",
    "            rank = int(line.split()[3])\n",
    "            \n",
    "            if rank <= max_rank:\n",
    "\n",
    "                if query not in run_dict:\n",
    "                    run_dict[query] = []\n",
    "                run_dict[query].append(doc_id)\n",
    "                doc_ids_list.append(doc_id)\n",
    "                \n",
    "    query_list  = sorted(list(run_dict.keys()))\n",
    "    \n",
    "    doc_ids_list = list(set(doc_ids_list))\n",
    "    print(\"doc_ids_list len = {}\".format(len(doc_ids_list)))\n",
    "    dataset_map = df_entity[df_entity['content_id'].isin(doc_ids_list)].select(\"content_id\", \"entities\").set_index('content_id').to_dict()\n",
    "    print(\"doc_ids_list len = {}\".format(len(dataset_map)))\n",
    "\n",
    "    print(dataset_map)\n",
    "    \n",
    "#     df_entity = \n",
    "#     for query_i, query in enumerate(query_list):\n",
    "#         print(\"Processing {} ({} / {})\".format(query, query_i+1, len(query_list)))\n",
    "        \n",
    "#         path = base_dir + '{}_entities.json'.format(query_i)\n",
    "#         query_json = {}\n",
    "#         df_dataset = df_entity[df_entity['content_id'].isin(run_dict[query])].select(\"content_id\", \"entities\")\n",
    "#         for row in df_dataset.collect():\n",
    "#             print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi_task_merge_env",
   "language": "python",
   "name": "multi_task_merge_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
