{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark_processing.ranking import get_paragraph_data_from_run_file, get_ranked_entities_from_paragraph_data\n",
    "from protocol_buffers import document_pb2\n",
    "\n",
    "from pyspark.sql.types import StringType, ArrayType, FloatType\n",
    "from pyspark.sql.functions import udf, row_number, explode, desc, col\n",
    "\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_drive_gbs = 20\n",
    "spark_executor_gbs = 3\n",
    "cores = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n//////// RUNNING WITH CORES {} //////////'.format(cores))\n",
    "spark = SparkSession.\\\n",
    "    builder\\\n",
    "    .appName('test')\\\n",
    "    .master('local[{}]'.format(cores)) \\\n",
    "    .config(\"spark.driver.memory\", '{}g'.format(spark_drive_gbs)) \\\n",
    "    .config(\"spark.executor.memory\", '{}g'.format(spark_executor_gbs)) \\\n",
    "    .config(\"spark.driver.maxResultSize\", '{}g'.format(spark_drive_gbs)) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_ids_maps(xml_topics_path, rank_type='passage'):\n",
    "    \"\"\" Build dict map from intermediate ids to Washington Post ids {intermediate_id: passage_id} \"\"\"\n",
    "    passage_id_map = {}\n",
    "    entity_id_map = {}\n",
    "    with open(xml_topics_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            # Passage intermediate_id\n",
    "            if '<num>' in line:\n",
    "                start_i = [m.span() for m in re.finditer('<num> Number: ', line)][0][1]\n",
    "                end_i = [m.span() for m in re.finditer(' </num>', line)][0][0]\n",
    "                passage_temp_id = line[start_i:end_i]\n",
    "            # Passage id\n",
    "            if '<docid>' in line:\n",
    "                start_i = [m.span() for m in re.finditer('<docid>', line)][0][1]\n",
    "                end_i = [m.span() for m in re.finditer('</docid>', line)][0][0]\n",
    "                passage_id = line[start_i:end_i]\n",
    "                passage_id_map[passage_temp_id] = passage_id\n",
    "\n",
    "            if rank_type == 'entity':\n",
    "                # Entity intermediate_id\n",
    "                if '<id>' in line:\n",
    "                    start_i = [m.span() for m in re.finditer('<id> ', line)][0][1]\n",
    "                    end_i = [m.span() for m in re.finditer(' </id>', line)][0][0]\n",
    "                    entity_temp_id = line[start_i:end_i]\n",
    "                # Entity id\n",
    "                if '<link>' in line:\n",
    "                    start_i = [m.span() for m in re.finditer('<link>', line)][0][1]\n",
    "                    end_i = [m.span() for m in re.finditer('</link>', line)][0][0]\n",
    "                    entity_id = line[start_i:end_i]\n",
    "                    entity_id_map[entity_temp_id] = entity_id\n",
    "\n",
    "    return passage_id_map, entity_id_map\n",
    "\n",
    "def get_top_100_rank(spark, run_path, rank_type='entity', k=100, xml_topics_path=None):\n",
    "    \"\"\"\"\"\"\n",
    "    if xml_topics_path == None:\n",
    "        passage_id_map, entity_id_map = None, None\n",
    "    else:\n",
    "        passage_id_map, entity_id_map = get_news_ids_maps(xml_topics_path=xml_topics_path, rank_type=rank_type)\n",
    "\n",
    "    data = []\n",
    "    with open(run_path, 'r', encoding='utf-8') as f_run:\n",
    "        for line in f_run:\n",
    "            query, _, doc_id, rank, _, _ = line.split()\n",
    "            if int(rank) <= k:\n",
    "                if rank_type == 'passage':\n",
    "                    if passage_id_map != None:\n",
    "                        if query in passage_id_map:\n",
    "                            data.append([passage_id_map[str(query)], str(doc_id), int(rank)])\n",
    "                    else:\n",
    "                        data.append([str(query), str(doc_id), int(rank)])\n",
    "                elif rank_type == 'entity':\n",
    "                    if entity_id_map != None:\n",
    "                        if query in entity_id_map:\n",
    "                            data.append([entity_id_map[str(query)], str(doc_id), int(rank)])\n",
    "                    else:\n",
    "                        data.append([str(query), str(doc_id), int(rank)])\n",
    "    return spark.createDataFrame(data, [\"query\", \"doc_id\", \"{}_rank\".format(rank_type)])\n",
    "\n",
    "\n",
    "@udf(returnType=FloatType())\n",
    "def get_entity_links_count(entity_link_ids):\n",
    "    return float(len(entity_link_ids))\n",
    "\n",
    "\n",
    "@udf(returnType=ArrayType(StringType()))\n",
    "def get_synthetic_entity_link_ids_passage(article_bytearray):\n",
    "    \"\"\"\"\"\"\n",
    "    if article_bytearray == None:\n",
    "        return []\n",
    "    article = pickle.loads(article_bytearray)\n",
    "    synthetic_entity_links = document_pb2.Document.FromString(article).document_contents[0].rel_entity_links\n",
    "    return [str(s.entity_id) for s in synthetic_entity_links]\n",
    "\n",
    "\n",
    "@udf(returnType=ArrayType(StringType()))\n",
    "def get_synthetic_entity_link_ids_entity(doc_bytearray):\n",
    "    \"\"\"\"\"\"\n",
    "    if doc_bytearray == None:\n",
    "        return []\n",
    "    doc = pickle.loads(doc_bytearray)\n",
    "    synthetic_entity_links = []\n",
    "    for document_content in document_pb2.Document.FromString(doc).document_contents:\n",
    "        synthetic_entity_links += document_content.synthetic_entity_links\n",
    "    return [str(s.entity_id) for s in synthetic_entity_links]\n",
    "\n",
    "def get_passage_df(spark, passage_run_path, xml_topics_path, passage_parquet_path):\n",
    "    \"\"\"\"\"\"\n",
    "    passage_rank_df = get_top_100_rank(spark=spark,\n",
    "                                       run_path=passage_run_path,\n",
    "                                       rank_type='passage',\n",
    "                                       k=100,\n",
    "                                       xml_topics_path=xml_topics_path)\n",
    "    passage_df = spark.read.parquet(passage_parquet_path).select(\"doc_id\", \"article_bytearray\")\n",
    "    passage_df_with_entity_links = passage_df.withColumn(\"entity_links\", get_synthetic_entity_link_ids_passage(\"article_bytearray\"))\n",
    "    passage_df_with_entity_links_counts = passage_df_with_entity_links.withColumn(\"entity_links_count\", get_entity_links_count(\"entity_links\"))\n",
    "    passage_join_df = passage_rank_df.join(passage_df_with_entity_links_counts, on=['doc_id'], how='left')\n",
    "    passage_join_df_with_counts_exploded = passage_join_df.select(\"query\", \"doc_id\", \"passage_rank\", \"entity_links_count\", explode(\"entity_links\").alias(\"entity_id\"))\n",
    "    return passage_join_df_with_counts_exploded\n",
    "\n",
    "\n",
    "def get_entity_df(spark, entity_run_path, entity_parquet_path, xml_topics_path):\n",
    "    \"\"\"\"\"\"\n",
    "    entity_rank_df = get_top_100_rank(spark=spark,\n",
    "                                      run_path=entity_run_path,\n",
    "                                      rank_type='entity',\n",
    "                                      k=100,\n",
    "                                      xml_topics_path=xml_topics_path)\n",
    "    print(entity_rank_df)\n",
    "    entity_df = spark.read.parquet(entity_parquet_path).select(col(\"page_id\").alias(\"doc_id\"), \"doc_bytearray\")\n",
    "    entity_df.printSchema\n",
    "    entity_join_df = entity_rank_df.join(entity_df, on=['doc_id'], how='left')\n",
    "    #entity_df_with_entity_links = entity_join_df.withColumn(\"entity_links\", get_synthetic_entity_link_ids_entity(\"doc_bytearray\"))\n",
    "    entity_df_with_entity_links_reduced = entity_join_df.select(col(\"doc_id\").alias(\"entity_id\"), \"query\", \"entity_rank\")\n",
    "    return entity_df_with_entity_links_reduced\n",
    "\n",
    "def build_news_graph(spark, passage_run_path, passage_xml_topics_path, passage_parquet_path, entity_run_path,\n",
    "                   entity_parquet_path, entity_xml_topics_path):\n",
    "    print(\"BUILDING PASSAGE DF\")\n",
    "    passage_df = get_passage_df(spark=spark,\n",
    "                                passage_run_path=passage_run_path,\n",
    "                                xml_topics_path=passage_xml_topics_path,\n",
    "                                passage_parquet_path=passage_parquet_path)\n",
    "    passage_df.show()\n",
    "\n",
    "    print(\"BUILDING ENTITY DF\")\n",
    "    entity_df = get_entity_df(spark=spark,\n",
    "                              entity_run_path=entity_run_path,\n",
    "                              xml_topics_path=entity_xml_topics_path,\n",
    "                              entity_parquet_path=entity_parquet_path)\n",
    "    entity_df.show()\n",
    "\n",
    "    print(\"JOINING\")\n",
    "    df = passage_df.join(entity_df, on=['query', 'entity_id'], how='left').fillna(0.0)\n",
    "    df.show()\n",
    "\n",
    "    print(\"BUILDING GRAPH WEIGHTS\")\n",
    "    @udf(returnType=FloatType())\n",
    "    def get_graph_weight(entity_links_count):\n",
    "        return 1 / entity_links_count\n",
    "\n",
    "    @udf(returnType=FloatType())\n",
    "    def get_passage_score(passage_rank):\n",
    "        return 1 / passage_rank\n",
    "\n",
    "#     @udf(returnType=FloatType())\n",
    "#     def get_norm_score(score, norm_factor):\n",
    "#         if (score == 0) or (norm_factor == 0):\n",
    "#             return 0.0\n",
    "#         return score / norm_factor\n",
    "\n",
    "    @udf(returnType=FloatType())\n",
    "    def get_entity_score(sum_graph_wight, passage_score):\n",
    "        return sum_graph_wight * passage_score\n",
    "\n",
    "    df_weighting = df.withColumn(\"graph_weigh\", get_graph_weight(\"entity_links_count\"))\n",
    "    df_entity_rank = df_weighting.groupBy(\"query\", \"entity_id\", \"passage_rank\").agg({\"graph_weigh\": \"sum\"}).fillna(0.0)\n",
    "\n",
    "    df_norm = df_entity_rank.groupBy(\"query\").agg({\"sum(graph_weigh)\": \"sum\"})\n",
    "    df_entity_rank_with_norm = df_entity_rank.join(df_norm, on=[\"query\"])\n",
    "    #df_entity_rank_with_norm_score = df_entity_rank_with_norm.withColumn(\"norm_score\", get_norm_score(\"sum(graph_weigh)\", \"sum(sum(graph_weigh))\"))\n",
    "    df_entity_rank_with_passage_score = df_entity_rank_with_norm_score.withColumn(\"entity_score\", get_passage_score(\"sum(sum(graph_weigh))\", \"passage_rank\"))\n",
    "\n",
    "    df_entity_rank_with_passage_score.show()\n",
    "    return df_entity_rank_with_passage_score.toPandas().sort_values([\"query\", \"passage_score\"], ascending=False)\n",
    "#df = get_paragraph_data_from_run_file(spark, run_path, para_path, max_counter=10000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "passage_run_path = '/nfs/trec_news_track/runs/anserini/background_2018/anserini.bm5.default.run'\n",
    "passage_xml_topics_path = '/nfs/trec_news_track/data/2018/newsir18-topics.txt'\n",
    "passage_parquet_path = '/nfs/trec_news_track/index/2018_bm25_rm3_chunks_full_v1/'\n",
    "entity_run_path = '/nfs/trec_news_track/runs/anserini/entity_2018/entity_ranking_BM25_1000.run'\n",
    "entity_parquet_path = '/nfs/trec_car/data/test_entity/full_data_v3_with_datasets/'\n",
    "entity_xml_topics_path = None\n",
    "\n",
    "build_news_graph(spark=spark, \n",
    "                 passage_run_path=passage_run_path, \n",
    "                 passage_xml_topics_path=passage_xml_topics_path, \n",
    "                 passage_parquet_path=passage_parquet_path, \n",
    "                 entity_run_path=entity_run_path,\n",
    "                 entity_parquet_path=entity_parquet_path, \n",
    "                 entity_xml_topics_path=entity_xml_topics_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi_task_merge_env",
   "language": "python",
   "name": "multi_task_merge_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
