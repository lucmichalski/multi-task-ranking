{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoderLayer, TransformerEncoder\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler, RandomSampler\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MUTANT(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model=768, seq_len=16, dropout=0.1):\n",
    "        super(MUTANT,self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.token_type_embeddings = nn.Embedding(3, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=2)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        \n",
    "        # Scoreing heads\n",
    "        self.head_passage = nn.Linear(d_model, 1)\n",
    "        self.head_entity = nn.Linear(d_model, 1)\n",
    "\n",
    "        \n",
    "    def forward(self, input_CLSs, type_mask=None):\n",
    "        # input_CLSs -> [seq_len, batch_size, d_model]\n",
    "        # type_mask -> [seq_len, batch_size] 0 or 1 for different types\n",
    "        \n",
    "        if isinstance(type_mask, torch.Tensor):\n",
    "                  token_type_embeddings = self.token_type_embeddings(type_mask)\n",
    "            print('----- token_type_embeddings -----')\n",
    "            print(token_type_embeddings.shape)\n",
    "            print(token_type_embeddings)\n",
    "            \n",
    "            input_CLSs = input_CLSs + token_type_embeddings \n",
    "            print('----- input_CLSs -----')\n",
    "            print(input_CLSs.shape)\n",
    "            print(input_CLSs)\n",
    "        \n",
    "            # Build padding masks i.e. type_mask == 0.\n",
    "            #src_key_padding_mask = (type_mask > 0).type(torch.int).T\n",
    "            src_key_padding_mask = (type_mask > 0).T\n",
    "            print('----- src_key_padding_mask -----')\n",
    "            print(src_key_padding_mask.shape)\n",
    "            print(src_key_padding_mask)\n",
    "            \n",
    "            # Forward pass of Transformer encoder.\n",
    "            output_CLSs = self.transformer_encoder(input_CLSs, src_key_padding_mask=src_key_padding_mask)\n",
    "            print('----- output_CLSs -----')\n",
    "            print(output_CLSs)\n",
    "\n",
    "            # Ensure Passage and Entity heads score correct mask type i.e. passage == 1 & entity = 2. \n",
    "            passage_mask = (type_mask == 1).type(torch.int).unsqueeze(-1)\n",
    "            entity_mask = (type_mask == 2).type(torch.int).unsqueeze(-1)\n",
    "            entity_output = self.head_entity(output_CLSs) * entity_mask\n",
    "            passage_output = self.head_passage(output_CLSs) * passage_mask\n",
    "        \n",
    "            output = passage_output+entity_output\n",
    "\n",
    "        else:\n",
    "            \n",
    "            output_CLSs = self.transformer_encoder(input_CLSs)\n",
    "#             print('----- output_CLSs -----')\n",
    "#             print(output_CLSs)\n",
    "\n",
    "            output = self.head_entity(output_CLSs)        \n",
    "      \n",
    "#         print('----- output -----')\n",
    "#         print(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def get_device(self):\n",
    "        return next(self.parameters()).device\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = MUTANT(d_model=10, seq_len=6, dropout=0.1)\n",
    "# lr = 0.001\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss\n",
    "# train_loss_total = 0.0\n",
    "\n",
    "# model.train()\n",
    "# bag_of_CLS = torch.rand(6, 3, 10) # [seq_len, batch_size, d_model]\n",
    "# type_mask = torch.tensor([[1,1,1],\n",
    "#                           [2,2,2],\n",
    "#                           [2,2,2],\n",
    "#                           [2,2,0],\n",
    "#                           [2,2,0],\n",
    "#                           [0,0,0]]) # [seq_len, batch_size]\n",
    "\n",
    "# labels = torch.tensor([[[1.0],[0.0],[1.0]],\n",
    "#                         [[0.0],[0.0],[0.0]],\n",
    "#                         [[1.0],[0.0],[1.0]],\n",
    "#                         [[0.0],[1.0],[0.0]],\n",
    "#                         [[0.0],[0.0],[0.0]],\n",
    "#                         [[0.0],[0.0],[0.0]]]) # [seq_len, batch_size]\n",
    "# for i in range(100):\n",
    "#     # ========================================\n",
    "#     #               Training\n",
    "#     # ========================================\n",
    "#     model.zero_grad()\n",
    "#     outputs = model.forward(bag_of_CLS, type_mask=type_mask)\n",
    "\n",
    "#     # Calculate Loss: softmax --> cross entropy loss\n",
    "#     loss = loss_func(outputs, labels)\n",
    "#     # Getting gradients w.r.t. parameters\n",
    "#     loss.sum().backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "\n",
    "#     train_loss_total += loss.sum().item()\n",
    "    \n",
    "#     if i % 10 == 0:\n",
    "#         print('--------')\n",
    "#         print(train_loss_total/(1+i))\n",
    "#         print(labels)\n",
    "#         print(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/nfs/trec_news_track/data/5_fold/scaled_5fold_0_data/mutant_data/valid/0_mutant_max.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-4c444b7d8f5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/nfs/trec_news_track/data/5_fold/scaled_5fold_0_data/mutant_data/valid/0_mutant_max.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/nfs/trec_news_track/data/5_fold/scaled_5fold_0_data/mutant_data/valid/0_mutant_max.json'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-93fea9cdd93b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0md_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m768\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mpassage_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'query'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'passage'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mseq_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mseq_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'd' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "bag_of_CLS = []\n",
    "labels = []\n",
    "type_mask = []\n",
    "max_seq_len = 16\n",
    "batch_size = 32\n",
    "d_model = 768\n",
    "dir_path = '/nfs/trec_news_track/data/5_fold/scaled_5fold_0_data/mutant_data/train/'\n",
    "file_name = '_mutant_max.json'\n",
    "doc_to_entity_map_path = '/nfs/trec_news_track/data/5_fold/scaled_5fold_0_data/doc_to_entity_map.json'\n",
    "\n",
    "with open(doc_to_entity_map_path, 'r') as f:\n",
    "    doc_to_entity_map = json.load(f)\n",
    "\n",
    "for path in [dir_path + f for f in os.listdir(dir_path) if file_name in f]:\n",
    "    with open(path, 'r') as f:\n",
    "        d = json.load(f)\n",
    "    for passage_id in d['query']['passage'].keys():\n",
    "        seq_cls = []\n",
    "        seq_labels = []\n",
    "        seq_mask = []\n",
    "\n",
    "        passage_cls = d['query']['passage'][passage_id]['cls_token']\n",
    "        passage_relevant = d['query']['passage'][passage_id]['relevant']\n",
    "        seq_cls.append(passage_cls)\n",
    "        seq_labels.append([passage_relevant])\n",
    "        seq_mask.append(1)\n",
    "        \n",
    "        if passage_id in doc_to_entity_map:\n",
    "            entity_id_list = doc_to_entity_map[passage_id]\n",
    "            entity_id_list_sorted = [elem for count, elem in sorted(((entity_id_list.count(e), e) for e in set(entity_id_list)), reverse=True)]\n",
    "            for entity_id in entity_id_list_sorted:\n",
    "                if len(seq_mask) < max_seq_len:\n",
    "                    entity_cls = d['query']['passage'][passage_id]['entity'][entity_id]['cls_token']\n",
    "                    entity_relevant = d['query']['passage'][passage_id]['entity'][entity_id]['relevant']\n",
    "                    seq_cls.append(entity_cls)\n",
    "                    seq_labels.append([entity_relevant])\n",
    "                    seq_mask.append(2)\n",
    "\n",
    "        else: \n",
    "            #print('{} not in doc_to_entity_map'.format(passage_id))\n",
    "            for entity_id in d['query']['passage'][passage_id]['entity']:\n",
    "                if len(seq_mask) < max_seq_len:\n",
    "                    entity_cls = d['query']['passage'][passage_id]['entity'][entity_id]['cls_token']\n",
    "                    entity_relevant = d['query']['passage'][passage_id]['entity'][entity_id]['relevant']\n",
    "                    seq_cls.append(entity_cls)\n",
    "                    seq_labels.append([entity_relevant])\n",
    "                    seq_mask.append(2)\n",
    "\n",
    "        if len(seq_mask) < max_seq_len:\n",
    "            padding_len = max_seq_len - len(seq_mask)\n",
    "            for i in range(padding_len):\n",
    "                seq_cls.append([0]*768)\n",
    "                seq_labels.append([0])\n",
    "                seq_mask.append(0)\n",
    "\n",
    "        bag_of_CLS.append(seq_cls) \n",
    "        labels.append(seq_labels)\n",
    "        type_mask.append(seq_mask)\n",
    "        break\n",
    "    break\n",
    "    \n",
    "        \n",
    "bag_of_CLS_tensor = torch.tensor(bag_of_CLS)\n",
    "type_mask_tensor = torch.tensor(type_mask)\n",
    "labels_tensor = torch.tensor(labels)\n",
    "print(bag_of_CLS_tensor.shape, type_mask_tensor.shape, labels_tensor.shape)\n",
    "\n",
    "train_dataset = TensorDataset(bag_of_CLS_tensor, type_mask_tensor, labels_tensor)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, sampler=SequentialSampler(train_dataset), batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-8e0a2dbceb7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtrain_loss_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-------------------------------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-------------------------------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data_loader' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = MUTANT(d_model=d_model, seq_len=max_seq_len, dropout=0.1)\n",
    "\n",
    " # Use GPUs if available.\n",
    "if torch.cuda.is_available():\n",
    "    # Tell PyTorch to use the GPU.\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU: {}'.format(torch.cuda.get_device_name(0)))\n",
    "    model.cuda()\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "# Otherwise use CPU.\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "for i in range(100):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss\n",
    "    train_loss_total = 0.0\n",
    "\n",
    "    model.train()\n",
    "    train_loss_total = 0\n",
    "    for i_train, train_batch in enumerate(train_data_loader):\n",
    "#         print('-------------------------------------')\n",
    "#         print('-------------------------------------')\n",
    "#         print('-------------------------------------')\n",
    "\n",
    "       \n",
    "        bag_of_CLS, type_mask, labels = train_batch\n",
    "#         bag_of_CLS = bag_of_CLS.view(max_seq_len,batch_size,d_model)\n",
    "#         type_mask = type_mask.view(max_seq_len,batch_size)\n",
    "#         labels = labels.view(max_seq_len,batch_size,1)\n",
    "        bag_of_CLS = torch.reshape(bag_of_CLS, (max_seq_len, bag_of_CLS.shape[0], d_model))\n",
    "        type_mask = torch.reshape(type_mask, (max_seq_len, type_mask.shape[0]))\n",
    "        labels = torch.reshape(labels, (max_seq_len, labels.shape[0], 1))\n",
    "        \n",
    "        print('----- batch -----')\n",
    "        print(bag_of_CLS.shape)\n",
    "        print(bag_of_CLS)\n",
    "        print(type_mask.shape)\n",
    "        print(type_mask)  \n",
    "        print(labels.shape)\n",
    "        print(labels)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        outputs = model.forward(bag_of_CLS.to(device), type_mask=type_mask.to(device))\n",
    "#         print(outputs)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = loss_func(outputs.cpu(), labels)\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.sum().backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        train_loss_total += loss.sum().item()\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(train_loss_total / len(train_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ['c','a', 'b', 'c', 'd', 'a', 'a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "y = collections.Counter(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "c\n",
      "d\n",
      "b\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in [elem for count, elem in sorted(((x.count(e), e) for e in set(x)), reverse=True)]:\n",
    "    print(i)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "bag_of_CLS = torch.rand(3, 2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5678, 0.5374, 0.3939, 0.5543],\n",
       "         [0.0576, 0.5003, 0.1142, 0.4358]],\n",
       "\n",
       "        [[0.7278, 0.0719, 0.7973, 0.0197],\n",
       "         [0.9373, 0.3528, 0.3551, 0.4850]],\n",
       "\n",
       "        [[0.5901, 0.5281, 0.8237, 0.7881],\n",
       "         [0.8297, 0.1818, 0.4055, 0.3765]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_CLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5678, 0.5374, 0.3939, 0.5543],\n",
       "         [0.0576, 0.5003, 0.1142, 0.4358],\n",
       "         [0.7278, 0.0719, 0.7973, 0.0197]],\n",
       "\n",
       "        [[0.9373, 0.3528, 0.3551, 0.4850],\n",
       "         [0.5901, 0.5281, 0.8237, 0.7881],\n",
       "         [0.8297, 0.1818, 0.4055, 0.3765]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.reshape(bag_of_CLS, (2,3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi_task_merge_env",
   "language": "python",
   "name": "multi_task_merge_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
