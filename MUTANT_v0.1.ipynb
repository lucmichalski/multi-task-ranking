{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoderLayer, TransformerEncoder\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler, RandomSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MUTANT(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model=768, seq_len=16, dropout=0.1):\n",
    "        super(MUTANT,self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.token_type_embeddings = nn.Embedding(3, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=2)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        \n",
    "        # Scoreing heads\n",
    "        self.head_passage = nn.Linear(d_model, 1)\n",
    "        self.head_entity = nn.Linear(d_model, 1)\n",
    "\n",
    "        \n",
    "    def forward(self, input_CLSs, type_mask=None):\n",
    "        # input_CLSs -> [seq_len, batch_size, d_model]\n",
    "        # type_mask -> [seq_len, batch_size] 0 or 1 for different types\n",
    "        \n",
    "        if isinstance(type_mask, torch.Tensor):\n",
    "            token_type_embeddings = self.token_type_embeddings(type_mask)\n",
    "            print('----- token_type_embeddings -----')\n",
    "            print(token_type_embeddings)\n",
    "            \n",
    "            input_CLSs = input_CLSs + token_type_embeddings \n",
    "            print('----- input_CLSs -----')\n",
    "            print(input_CLSs)\n",
    "        \n",
    "            # Build padding masks i.e. type_mask == 0.\n",
    "            #src_key_padding_mask = (type_mask > 0).type(torch.int).T\n",
    "            src_key_padding_mask = (type_mask > 0).T\n",
    "            print('----- src_key_padding_mask -----')\n",
    "        \n",
    "            # Forward pass of Transformer encoder.\n",
    "            output_CLSs = self.transformer_encoder(input_CLSs, src_key_padding_mask=src_key_padding_mask)\n",
    "            print('----- output_CLSs -----')\n",
    "            print(output_CLSs)\n",
    "\n",
    "            # Ensure Passage and Entity heads score correct mask type i.e. passage == 1 & entity = 2. \n",
    "            passage_mask = (type_mask == 1).type(torch.int).unsqueeze(-1)\n",
    "            entity_mask = (type_mask == 2).type(torch.int).unsqueeze(-1)\n",
    "            entity_output = self.head_entity(output_CLSs) * entity_mask\n",
    "            passage_output = self.head_passage(output_CLSs) * passage_mask\n",
    "        \n",
    "            output = passage_output+entity_output\n",
    "\n",
    "        else:\n",
    "            \n",
    "            output_CLSs = self.transformer_encoder(input_CLSs)\n",
    "            print('----- output_CLSs -----')\n",
    "            print(output_CLSs)\n",
    "\n",
    "            output = self.head_entity(output_CLSs)        \n",
    "      \n",
    "        print('----- output -----')\n",
    "        print(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def get_device(self):\n",
    "        return next(self.parameters()).device\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = MUTANT(d_model=10, seq_len=6, dropout=0.1)\n",
    "# lr = 0.001\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss\n",
    "# train_loss_total = 0.0\n",
    "\n",
    "# model.train()\n",
    "# bag_of_CLS = torch.rand(6, 3, 10) # [seq_len, batch_size, d_model]\n",
    "# type_mask = torch.tensor([[1,1,1],\n",
    "#                           [2,2,2],\n",
    "#                           [2,2,2],\n",
    "#                           [2,2,0],\n",
    "#                           [2,2,0],\n",
    "#                           [0,0,0]]) # [seq_len, batch_size]\n",
    "\n",
    "# labels = torch.tensor([[[1.0],[0.0],[1.0]],\n",
    "#                         [[0.0],[0.0],[0.0]],\n",
    "#                         [[1.0],[0.0],[1.0]],\n",
    "#                         [[0.0],[1.0],[0.0]],\n",
    "#                         [[0.0],[0.0],[0.0]],\n",
    "#                         [[0.0],[0.0],[0.0]]]) # [seq_len, batch_size]\n",
    "# for i in range(100):\n",
    "#     # ========================================\n",
    "#     #               Training\n",
    "#     # ========================================\n",
    "#     model.zero_grad()\n",
    "#     outputs = model.forward(bag_of_CLS, type_mask=type_mask)\n",
    "\n",
    "#     # Calculate Loss: softmax --> cross entropy loss\n",
    "#     loss = loss_func(outputs, labels)\n",
    "#     # Getting gradients w.r.t. parameters\n",
    "#     loss.sum().backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "\n",
    "#     train_loss_total += loss.sum().item()\n",
    "    \n",
    "#     if i % 10 == 0:\n",
    "#         print('--------')\n",
    "#         print(train_loss_total/(1+i))\n",
    "#         print(labels)\n",
    "#         print(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/nfs/trec_news_track/data/5_fold/scaled_5fold_0_data/mutant_data/valid/0_mutant_max.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-4c444b7d8f5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/nfs/trec_news_track/data/5_fold/scaled_5fold_0_data/mutant_data/valid/0_mutant_max.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/nfs/trec_news_track/data/5_fold/scaled_5fold_0_data/mutant_data/valid/0_mutant_max.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "path = '/nfs/trec_news_track/data/5_fold/scaled_5fold_0_data/mutant_data/valid/0_mutant_max.json'\n",
    "\n",
    "with open(path, 'r') as f:\n",
    "    d = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-93fea9cdd93b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0md_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m768\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mpassage_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'query'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'passage'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mseq_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mseq_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'd' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "bag_of_CLS = []\n",
    "labels = []\n",
    "type_mask = []\n",
    "max_seq_len = 16\n",
    "batch_size = 2\n",
    "d_model = 768\n",
    "\n",
    "for passage_id in d['query']['passage'].keys():\n",
    "    seq_cls = []\n",
    "    seq_labels = []\n",
    "    seq_mask = []\n",
    "        \n",
    "    passage_cls = d['query']['passage'][passage_id]['cls_token']\n",
    "    passage_relevant = d['query']['passage'][passage_id]['relevant']\n",
    "    seq_cls.append(passage_cls)\n",
    "    seq_labels.append([passage_relevant])\n",
    "    seq_mask.append(1)\n",
    "\n",
    "    for entity_id in d['query']['passage'][passage_id]['entity']:\n",
    "        if len(seq_mask) < max_seq_len:\n",
    "            entity_cls = d['query']['passage'][passage_id]['entity'][entity_id]['cls_token']\n",
    "            entity_relevant = d['query']['passage'][passage_id]['entity'][entity_id]['relevant']\n",
    "            seq_cls.append(entity_cls)\n",
    "            seq_labels.append([entity_relevant])\n",
    "            seq_mask.append(2)\n",
    "        else:\n",
    "            pass\n",
    "            #print('not enough max_seq_leng for: {} - entity: {}, total ents: {}'.format(passage_id, entity_id, len(d['query']['passage'][passage_id]['entity'])))\n",
    "            \n",
    "    if len(seq_mask) < max_seq_len:\n",
    "        padding_len = max_seq_len - len(seq_mask)\n",
    "        for i in range(padding_len):\n",
    "            seq_cls.append([0]*768)\n",
    "            seq_labels.append([0])\n",
    "            seq_mask.append(0)\n",
    "        \n",
    "    bag_of_CLS.append(seq_cls) \n",
    "    labels.append(seq_labels)\n",
    "    type_mask.append(seq_mask)\n",
    "    \n",
    "        \n",
    "bag_of_CLS_tensor = torch.tensor(bag_of_CLS)\n",
    "type_mask_tensor = torch.tensor(type_mask)\n",
    "labels_tensor = torch.tensor(labels)\n",
    "print(bag_of_CLS_tensor.shape, type_mask_tensor.shape, labels_tensor.shape)\n",
    "\n",
    "train_dataset = TensorDataset(bag_of_CLS_tensor, type_mask_tensor, labels_tensor)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, sampler=SequentialSampler(train_dataset), batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-8e0a2dbceb7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtrain_loss_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-------------------------------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-------------------------------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data_loader' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "model = MUTANT(d_model=d_model, seq_len=max_seq_len, dropout=0.1)\n",
    "lr = 0.001\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss\n",
    "train_loss_total = 0.0\n",
    "\n",
    "model.train()\n",
    "\n",
    "for i in range(1000):\n",
    "    train_loss_total = 0\n",
    "    for i_train, train_batch in enumerate(train_data_loader):\n",
    "        print('-------------------------------------')\n",
    "        print('-------------------------------------')\n",
    "        print('-------------------------------------')\n",
    "       \n",
    "        bag_of_CLS, type_mask, labels = train_batch\n",
    "#         bag_of_CLS = bag_of_CLS.view(max_seq_len,batch_size,d_model)\n",
    "#         type_mask = type_mask.view(max_seq_len,batch_size)\n",
    "#         labels = labels.view(max_seq_len,batch_size,1)\n",
    "        \n",
    "        print('----- batch -----')\n",
    "        print(bag_of_CLS.shape)\n",
    "        print(bag_of_CLS)\n",
    "        print(type_mask.shape)\n",
    "        print(type_mask)  \n",
    "        print(labels.shape)\n",
    "        print(labels)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        outputs = model.forward(bag_of_CLS, type_mask=None)\n",
    "#         print(outputs)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = loss_func(outputs, labels)\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.sum().backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        train_loss_total += loss.sum().item()\n",
    "        break\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(train_loss_total / len(train_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi_task_merge_env",
   "language": "python",
   "name": "multi_task_merge_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
